import json
import pandas as pd
import unicodedata
import re


# ========================================================================
# 1. H√ÄM X·ª¨ L√ù CHU·ªñI C∆† B·∫¢N (ICON, KHO·∫¢NG TR·∫ÆNG, VI·∫æT HOA‚Ä¶)
# ========================================================================

def remove_icons(text: str) -> str:
    """X√≥a emoji, k√Ω hi·ªáu thu·ªôc Unicode category 'Symbol'."""
    if not text:
        return text

    emoji_pattern = re.compile(
        "[" 
        "\U0001F600-\U0001F64F"
        "\U0001F300-\U0001F5FF"
        "\U0001F680-\U0001F6FF"
        "\U0001F700-\U0001F77F"
        "\U0001F780-\U0001F7FF"
        "\U0001F800-\U0001F8FF"
        "\U0001F900-\U0001F9FF"
        "\U0001FA00-\U0001FAFF"
        "\U00002700-\U000027BF"
        "\uFE0F"
        "]+",
        flags=re.UNICODE
    )
    text = emoji_pattern.sub("", text)

    text = "".join(ch for ch in text if not unicodedata.category(ch).startswith("S"))
    return re.sub(r"\s+", " ", text).strip()


def clean_json(data):
    """Duy·ªát to√†n b·ªô JSON v√† x√≥a icon."""
    if isinstance(data, dict):
        return {k: clean_json(v) for k, v in data.items()}
    if isinstance(data, list):
        return [clean_json(v) for v in data]
    if isinstance(data, str):
        return remove_icons(data)
    return data


def normalize_text(text: str) -> str:
    """Chu·∫©n h√≥a chu·ªói: x√≥a icon + x√≥a kho·∫£ng tr·∫Øng th·ª´a."""
    return re.sub(r"\s+", " ", remove_icons(text)).strip()


def normalize_name(text: str) -> str:
    """Vi·∫øt hoa ch·ªØ c√°i ƒë·∫ßu c·ªßa t√™n m√≥n ƒÉn."""
    if not text:
        return text
    return text.strip().capitalize()


# ========================================================================
# 2. CHU·∫®N H√ìA TH·ªúI GIAN & S·ªê NG∆Ø·ªúI ƒÇN
# ========================================================================

def normalize_cook_time(time_str):
    """Chuy·ªÉn '1 gi·ªù 20 ph√∫t' ‚Üí 80 (ph√∫t)."""
    if not time_str:
        return 0

    time_str = time_str.lower().strip()
    total = 0

    hours = re.findall(r"(\d+)\s*(gi·ªù|h)", time_str)
    minutes = re.findall(r"(\d+)\s*(ph√∫t|ph)", time_str)

    if hours:
        total += int(hours[0][0]) * 60
    if minutes:
        total += int(minutes[0][0])

    if total == 0:
        nums = re.findall(r"\d+", time_str)
        if nums:
            total = int(nums[0])

    return total


def normalize_servings(servings_str):
    """Chu·∫©n h√≥a kh·∫©u ph·∫ßn: '4‚Äì5 ng∆∞·ªùi' ‚Üí '4-5'."""
    if not servings_str:
        return ""

    servings_str = servings_str.lower().strip()
    match = re.findall(r"(\d+)\s*[-‚Äì]?\s*(\d+)?", servings_str)

    if match:
        left, right = match[0]
        return f"{left}-{right}" if right else left

    return ""


# ========================================================================
# 3. CHU·∫®N H√ìA ƒê∆†N V·ªä ƒêO L∆Ø·ªúNG
# ========================================================================

def normalize_unit(text):
    """Chu·∫©n h√≥a c√°c ƒë∆°n v·ªã vi·∫øt t·∫Øt: g ‚Üí gam, M ‚Üí mu·ªóng‚Ä¶"""
    if isinstance(text, list):
        return [normalize_unit(x) for x in text]

    if not isinstance(text, str):
        return text

    unit_map = {
        'm': 'mu·ªóng',
        'M': 'mu·ªóng',
        'g': 'gam',
        'kg': 'kilogram',
        'tr': 'tr√°i',
        'c': 'c·ªß',
        'qu': 'qu·∫£',
        'ml': 'ml',
    }

    for k, v in unit_map.items():
        text = re.sub(rf"(\d[\d\s./]*)\s*{k}\b", rf"\1 {v}", text)

    return text.strip()


# ========================================================================
# 4. CHU·∫®N H√ìA T√äN NGUY√äN LI·ªÜU (LO·∫†I BRAND, LO·∫†I M√î T·∫¢)
# ========================================================================

def normalize_ingredient_name(name: str) -> str:
    """X√≥a m√¥ t·∫£, th∆∞∆°ng hi·ªáu, gom nh√≥m t·ª´ ƒë·ªìng nghƒ©a."""
    if not name:
        return ""

    # 1. X√≥a icon + lowercase
    raw = remove_icons(name).lower()

    # 2. X√≥a k√Ω t·ª± ƒë·∫∑c bi·ªát
    raw = re.sub(r'[^\w\s]', '', raw)

    # # 3. Lo·∫°i b·ªè c√°c t·ª´ m√¥ t·∫£/th∆∞∆°ng hi·ªáu/kh√¥ng c·∫ßn thi·∫øt
    # ignore_terms = ["gia v·ªã", "ƒÉn k√®m", "trang tr√≠", "d√πng k√®m"]
    # for term in ignore_terms:
    #     raw = raw.replace(term, "")

    # 4. Lo·∫°i b·ªè ƒë·ªông t·ª´, h√†nh ƒë·ªông ch·∫ø bi·∫øn
    remove_verbs = [
        "gia v·ªã", "ƒÉn k√®m", "ƒÉn tr∆∞a k√®m", "ƒÉn t·ªëi k√®m", "trang tr√≠", "d√πng k√®m", "rau n√™m",
        "bƒÉm", "phi", "c·∫Øt", "xay", "lu·ªôc", "th√°i", "n∆∞·ªõng",
        "chi√™n", "h·∫•p", "tr·ª•ng", "l√°t", "nhuy·ªÖn", "ƒë·∫≠p d·∫≠p", "gi√£","ƒë·∫≠p gi·∫≠p",
        "ƒë·ªÉ r√°o", "t∆∞∆°i", "s·ª£i", "c·∫Øt s·ª£i", "h∆∞·ªùm", "poar√¥", "m·ªÅm", "tr√°ng m·ªèng",
        "b√≥c v·ªè", "non", "gi√†", "c·ªçng", "ch√≠n", 'b√†o', 'tr√°i', "nh·ªè", 
        "c√¢y", 't∆° m·ªÅm', "dƒÉm", "phil√™", "t√°ch v·ªè", "b√∫p", "kh√¥", 
        "l√†m s·∫°ch","gi√≤n", "n·∫°o", "c·ªçng to", "l·∫∑t s·∫°ch", "m·ªèng",
        "l·ªôt b·ªè da", "kh√¥", "c·∫°n", "nori vu√¥ng b·∫±ng mi·∫øng sandwich",
        "ng√¢m m·ªÅm", "b√†o m·ªèng", "s∆°","l√†m s·∫µn", "ng√¢m n·ªü", "ƒë√°t nh·ªè",
        "l·ªôt v·ªè", "s·ªë 1", "rang", "ta", "ngon", "c√≥ d·∫ßu", "d√∫n", "c√°c lo·∫°i",
        "que", "ch·∫ßn", "c·∫Øt h·∫°t l·ª±u", "h·∫°t l·ª±u", "h·ªôp", "m√†i nh·ªè", "c√≤n s·ªëng",
        "ng√¢m d·∫ßu", "tr√°i", "ƒë√®o", "h√¨nh thoi", "nh·∫≠t", "ƒë√£ ng√¢m", "x·∫Øt", "l·∫°t", 
        "l·ªõn", "ng√¢m chua", "gi·∫£", "d·∫ªo th∆°m", "kh√¥ng h·∫°t", "nguy√™n h·∫°t", "g√≥c t∆∞",
        "nguy√™n li·ªáu", "b·ªè da", "lo·∫°i", "cac loai", "r√∫t x∆∞∆°ng", "ru·ªôt xanh", 
        "tr√≤n l√†m ƒë·∫ø b√°nh ti√™u", "h·ªôt", "ƒë·∫∑c ru·ªôt", "kh√¥ng da", "lo·∫°i", "s·∫µn",
        "ƒë·∫∑c", "nguy√™n v·ªè", "da", "th√¥ng th∆∞·ªùng", "nguy√™n con", "h·∫°t tr√≤n", "v·ª´a t·ªõi",
        "ajix·ªët", "ƒë√¥ng l·∫°nh", "ƒëa d·ª•ng", "ƒë√†", "t√πy √Ω kh√∫c", "t√πy √Ω", "th∆∞·ªùng", "Ajingon",
        "kh√∫c gi·ªØa", "to", "b√©", "b·ªè v·ªè t√°ch ƒë√¥i", "lon", "ƒë·ªÉ nguy√™n l√°", "m·ªçng", "kho·∫£ng",
        "l·ªçc x∆∞∆°ng", "b·ªè v·ªè", "g·ªçt v·ªè", "kh√∫c", "ch·ª´a ƒëu√¥i", "ng√¢m n∆∞·ªõc l·∫°nh", "ƒë·ªÉ ri√™ng g·ªëc v√† l√°",
        "d·∫πp", "b·ªè ƒëu√¥i", "g·ªçt s·∫°ch v·ªè", "th·∫£ v∆∞·ªùn", "ng√¢m", "√°p ch·∫£o", "ch·ª´a ƒëu√¥i", "h·∫°t c√≤n v·ªè", 
        ""
    ]
    for v in remove_verbs:
        raw = re.sub(rf"\b{v}\b", "", raw)

    # 5. Lo·∫°i b·ªè th∆∞∆°ng hi·ªáu
    brand_map = ["aji-ngon", "aji-no-moto", "ph√∫ sƒ©", "ajinomoto"]
    for b in brand_map:
        raw = raw.replace(b, "")

    # 6. Gom nh√≥m b·∫±ng replacements
    replacements = {
        "h·∫°t n√™m ajingon heo": "H·∫°t n√™m",
        "h·∫°t n√™m ajingon n·∫•m": "H·∫°t n√™m",
        "h·∫°t n√™m ajingon g√†": "H·∫°t n√™m",
        "b·ªôt ng·ªçt ajinomoto": "B·ªôt ng·ªçt",
        "ajinomoto gi·∫•m gao len men": "Gi·∫•m g·∫°o l√™n men",
        "n∆∞·ªõc t∆∞∆°ng ph√∫ sƒ©": "N∆∞·ªõc t∆∞∆°ng",
        "n∆∞·ªõc t∆∞∆°ng lisa" : "N∆∞·ªõc t∆∞∆°ng",
        "x·ªët t∆∞∆°ng ƒë·∫≠u n√†nh lisa": "X·ªët t∆∞∆°ng ƒë·∫≠u n√†nh",
        "x·ªët mayonnaise ajimayo v·ªã ng·ªçt d·ªãu": "X·ªët Mayonnaise",
        "x·ªët mayonnaise ajimayo v·ªã nguy√™n b·∫£n": "X·ªët Mayonnaise",
        "ajiquick b·ªôt": "B·ªôt chi√™n gi√≤n",
        "ajiquick b·ªôt t·∫©m": "B·ªôt chi√™n gi√≤n",
        "ajiquick b·ªôt t·∫©m kh√¥ gi√≤n": "B·ªôt chi√™n gi√≤n",
        "ajiquick b·ªôt gi√≤n": "B·ªôt chi√™n gi√≤n",
        "n√™m ajiquick l·∫©u" : "Gia v·ªã n√™m s·∫µn l·∫©u",
        "n√™m s·∫µn ajiquick l·∫©u" : "Gia v·ªã n√™m s·∫µn l·∫©u",
        "n√™m s·∫µn ajiquick th·ªãt kho" :"G√≥i gia v·ªã n√™m s·∫µn n·∫•u th·ªãt kho",
        "ƒë·∫ßu h√†nh v√† h√†nh t√≠m" : 'H√†nh', 
        "x·ªët d√πng ngay kho qu·∫πt" : "Kho qu·∫πt",
        "n√™m s·∫µn ajiquick ph·ªü b√≤" : "Gia v·ªã n√™m s·∫µn ph·ªü b√≤",
        "n√™m s·∫µn ajiquick b√∫n ri√™u cua" : "Gia v·ªã n√™m s·∫µn b√∫n ri√™u cua",
      

    }
    for k, v in replacements.items():
        # x√≥a space th·ª´a + lowercase tr∆∞·ªõc khi so s√°nh
        raw_cmp = re.sub(r'\s+', ' ', raw)
        if k in raw_cmp:
            return v

    # 7. Cleanup kho·∫£ng tr·∫Øng
    raw = re.sub(r'\s+', ' ', raw).strip()
    if not raw:
        return ""

    # 8. Vi·∫øt hoa ch·ªØ c√°i ƒë·∫ßu
    # return raw[0].upper() + raw[1:]
    return raw.lower()


# ========================================================================
# 5. T√ÅCH NGUY√äN LI·ªÜU ‚Üí (t√™n, s·ªë l∆∞·ª£ng)
# ========================================================================

def clean_name(name: str) -> str:
    """Chu·∫©n h√≥a t√™n nguy√™n li·ªáu cu·ªëi c√πng."""
    if not name:
        return ""

    name = re.sub(r"\(.*?\)", "", name)
    name = normalize_ingredient_name(name)
    return re.sub(r"\s+", " ", name).strip()


def detect_ingredient_parts(text: str):
    """T√°ch 1 d√≤ng nguy√™n li·ªáu ‚Üí (name, qty) chu·∫©n h√≥a n√¢ng cao."""
    text = text.strip()

    # --- 1. T√°ch n·∫øu c√≥ nhi·ªÅu nguy√™n li·ªáu b·∫±ng d·∫•u ph·∫©y (ch·ªâ l·∫•y ph·∫ßn ƒë·∫ßu v√¨ v√≤ng for x·ª≠ l√Ω t·ª´ng item) ---
    if "," in text:
        text = text.split(",")[0].strip()

    # --- 2. N·∫øu c√≥ d·∫•u ":" t√°ch name : quantity ---
    if ":" in text:
        name_part, qty_part = text.split(":", 1)
        name = clean_name(name_part)
        qty = qty_part.strip() or None

        # üî• CHU·∫®N H√ìA ƒê∆†N V·ªä  (th√™m d√≤ng n√†y)
        if qty:
            qty = normalize_unit(qty)

        return name, qty

    # --- 3. Regex t√¨m s·ªë l∆∞·ª£ng ---
    match = re.search(r"(\d[\d\s./]*\s*(?:g|gam|kg|ml|tr√°i|c√¢y|mu·ªóng|qu·∫£|l√°)?)", text, flags=re.I)

    if match:
        quantity = match.group(0).strip() or None
        name = text[:match.start()].strip()
        name = clean_name(name)

        # üî• CHU·∫®N H√ìA ƒê∆†N V·ªä (th√™m d√≤ng n√†y)
        if quantity:
            quantity = normalize_unit(quantity)

        return name, quantity

    # --- 4. Kh√¥ng t√¨m th·∫•y s·ªë l∆∞·ª£ng ‚Üí quantity = None ---
    name = clean_name(text)
    return name, None


def process_ingredients(ingredients):
    """Chuy·ªÉn list nguy√™n li·ªáu ‚Üí (list t√™n, list s·ªë l∆∞·ª£ng)."""
    names, quantities = [], []
    seen = set()  # d√πng ƒë·ªÉ lo·∫°i b·ªè tr√πng l·∫∑p

    for item in ingredients:
        name, qty = detect_ingredient_parts(item)
        if not name:
            continue  # skip empty
        if name not in seen:
            names.append(name)
            quantities.append(qty)
            seen.add(name)
        else:
            # n·∫øu mu·ªën g·ªôp qty tr√πng, x·ª≠ l√Ω ·ªü ƒë√¢y
            pass

    return names, quantities


# ========================================================================
# 6. PH√ÇN LO·∫†I M√ìN ƒÇN
# ========================================================================

def detect_category(name):
    name = name.lower()
    mapping = {
        "canh": "canh", "s√∫p": "s√∫p",
        "x√†o": "x√†o", "chi√™n": "chi√™n", "r√°n": "chi√™n",
        "kho": "kho", "rim": "rim", "om": "om",
        "n∆∞·ªõng": "n∆∞·ªõng", "h·∫•p": "h·∫•p", "lu·ªôc": "lu·ªôc",
        "l·∫©u": "l·∫©u", "ch√°o": "ch√°o",
        "g·ªèi": "g·ªèi", "salad": "salad",
        "cu·ªën": "cu·ªën", "nem": "nem", "ch·∫£": "ch·∫£",
        "b√∫n": "m√≥n n∆∞·ªõc", "ph·ªü": "m√≥n n∆∞·ªõc",
        "mi·∫øn": "m√≥n n∆∞·ªõc", "h·ªß ti·∫øu": "m√≥n n∆∞·ªõc",
        "ch√®": "ch√®", "kem": "tr√°ng mi·ªáng",
        "b√°nh": "b√°nh",
        "c√† ri": "c√† ri",
        "kim chi": "m√≥n H√†n", "tokbokki": "m√≥n H√†n",
        "sushi": "m√≥n Nh·∫≠t", "udon": "m√≥n Nh·∫≠t", "ramen": "m√≥n Nh·∫≠t",
        "tr·ªôn": "tr·ªôn",
        "x·ªët": "x·ªët"
    }
    for k, v in mapping.items():
        if k in name:
            return v.capitalize()
    return "m√≥n kh√°c"


# ========================================================================
# 7. H√ÄM CH√çNH X·ª¨ L√ù TO√ÄN B·ªò DATAFRAME
# ========================================================================

def process_and_export(raw_data, output_file):
    df = pd.DataFrame(raw_data)
    
    df["dish_name"] = df["dish_name"].apply(normalize_text).apply(normalize_name)

    df["ingredient_names"], df["ingredient_quantities"] = zip(
        *df["ingredients"].apply(process_ingredients)
    )

    df = df.drop(columns=["ingredients"])

    if "cooking_time" in df.columns:
        df["cooking_time"] = df["cooking_time"].apply(normalize_cook_time)

    if "servings" in df.columns:
        df["servings"] = df["servings"].apply(normalize_servings)

    df["category"] = df["dish_name"].apply(detect_category)

    if "url" in df.columns:
        df = df.drop_duplicates(subset=["url"])

    df = df.reset_index(drop=True)
    df["index"] = df.index + 1
    df["ingredient_count"] = df["ingredient_names"].apply(len)

    df.to_json(output_file, orient="records", indent=2, force_ascii=False)
    print("‚úÖ ƒê√£ xu·∫•t file", output_file)


# ========================================================================
# 8. CH·∫†Y TR·ª∞C TI·∫æP
# ========================================================================

if __name__ == "__main__":
    input_file = "./recipes_501_1000_raw.json"
    output_file = "./recipes_501_1000_cleaned.json"

    with open(input_file, "r", encoding="utf-8") as f:
        raw = json.load(f)

    cleaned = clean_json(raw)
    process_and_export(cleaned, output_file)

    # X√≥a escape \/ trong URL
    with open(output_file, "r", encoding="utf-8") as f:
        data = f.read().replace("\\/", "/")
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(data)

    print("üéâ Ho√†n t·∫•t.")
