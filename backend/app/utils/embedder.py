# from sentence_transformers import SentenceTransformer
# from tqdm import tqdm
# import pandas as pd
# import torch


# def load_vietnamese_embedding_model(device: str = None):
#     """
#     Load embedding model with GPU support if available.
#     """
#     model_name = "paraphrase-multilingual-MiniLM-L12-v2"
#     print(f"üîπ Loading model: {model_name}")

#     if device is None:
#         device = "cuda" if torch.cuda.is_available() else "cpu"

#     model = SentenceTransformer(model_name, device=device)
#     print(f"üëâ Using device: {device}")

#     return model


# def embed_texts(texts, model: SentenceTransformer, batch_size=64):
#     """
#     Encode texts efficiently with GPU + normalization.
#     """
#     return model.encode(
#         texts,
#         batch_size=batch_size,
#         show_progress_bar=True,
#         convert_to_tensor=False,  # ƒë·ªÉ l∆∞u pickle d·ªÖ
#         normalize_embeddings=True # embedding chu·∫©n h√≥a ‚Üí t·ªët cho kNN / cosine
#     )


# def generate_vietnamese_recipe_embeddings(df: pd.DataFrame):

#         model = load_vietnamese_embedding_model()
        
#         columns_to_embed = {
#             "ingredient_names": "ingredient_names_embedding",
#             "ingredient_quantities": "ingredient_quantities_embedding",
#             "dish_name": "dish_name_embedding"
#         }

#         for text_col, out_col in columns_to_embed.items():
#             print(f"Embedding column: {text_col}")
#             texts = df[text_col].fillna("").astype(str).tolist()

#             vectors = embed_texts(texts, model, batch_size=32)  # numpy array (n, 384)

#             # Fix ch√≠nh x√°c:
#             vectors_list = [v.tolist() for v in vectors]  # m·ªói row l√† list 384 float
#             df[out_col] = pd.Series(vectors_list, dtype=object)

#         return df



# if __name__ == "__main__":
#     df = pd.read_csv("./data/test_processed_recipes.csv")

#     df_emb = generate_vietnamese_recipe_embeddings(df)

#     output = "./data/recipes_embeddings.pkl"
#     df_emb.to_pickle(output)
#     print(f"‚úÖ Saved embeddings to {output}")


from sentence_transformers import SentenceTransformer
from tqdm import tqdm
import pandas as pd
import torch
from typing import List, Union


def load_vietnamese_embedding_model(device: str = None):
    """
    Load BGE embedding model with GPU support.
    """
    # üî• Model BGE cho ti·∫øng Vi·ªát (n√™n d√πng)
    model_name = "BAAI/bge-base-en-v1.5"   # ho·∫∑c "dinhnguyenhv/bge-vi-base" n·∫øu b·∫°n c√≥ b·∫£n VI-FT
    print(f"üîπ Loading BGE model: {model_name}")

    if device is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"

    model = SentenceTransformer(model_name, device=device)
    print(f"üëâ Using device: {device}")

    return model


# def embed_texts(texts, model: SentenceTransformer, batch_size=64):
#     """
#     Encode texts using BGE + normalization.
#     L∆ØU √ù: BGE khuy·∫øn ngh·ªã d√πng normalize_embeddings=True cho cosine + FAISS.
#     """
#     # V·ªõi BGE, ƒë·ªÉ k·∫øt qu·∫£ ƒë√∫ng, ph·∫£i th√™m prefix "query: " ho·∫∑c "passage: " (t√πy task)
#     # Nh∆∞ng ·ªü ƒë√¢y b·∫°n ƒëang encode d·ªØ li·ªáu (passages) ‚Üí d√πng prefix "passage: "
#     prefixed = [f"passage: {t}" for t in texts]

#     return model.encode(
#         prefixed,
#         batch_size=batch_size,
#         show_progress_bar=True,
#         convert_to_tensor=False,
#         normalize_embeddings=True
#     )

def embed_text(text: str, model: SentenceTransformer, text_type: str = "passage") -> List[float]:
    """
    T·∫°o embedding cho m·ªôt chu·ªói duy nh·∫•t s·ª≠ d·ª•ng m√¥ h√¨nh BGE.
    T·ª± ƒë·ªông th√™m prefix ('passage' ho·∫∑c 'query') v√† chu·∫©n h√≥a embedding.
    
    Args:
        text: Chu·ªói c·∫ßn embed.
        model: M√¥ h√¨nh SentenceTransformer (BGE).
        text_type: Lo·∫°i text, 'passage' (m·∫∑c ƒë·ªãnh) ho·∫∑c 'query'.
    
    Returns:
        List[float]: Vector embedding ƒë√£ chu·∫©n h√≥a.
    """
    # Th√™m prefix theo lo·∫°i text
    prefix = f"{text_type}: "
    
    # Encode text, normalize embedding ƒë·ªÉ d√πng cho cosine similarity ho·∫∑c FAISS
    embedding = model.encode(
        [prefix + text], 
        show_progress_bar=False, 
        normalize_embeddings=True
    )
    
    # Tr·∫£ v·ªÅ vector d·∫°ng list
    return embedding[0].tolist()


def embed_texts(texts: List[str], model: SentenceTransformer, batch_size: int = 32, text_type: str = "passage") -> List[List[float]]:
    """
    T·∫°o embedding cho danh s√°ch nhi·ªÅu chu·ªói s·ª≠ d·ª•ng m√¥ h√¨nh BGE theo batch.
    T·ª± ƒë·ªông th√™m prefix ('passage' ho·∫∑c 'query') v√† chu·∫©n h√≥a embedding.
    
    Args:
        texts: Danh s√°ch c√°c chu·ªói c·∫ßn embed.
        model: M√¥ h√¨nh SentenceTransformer (BGE).
        batch_size: S·ªë l∆∞·ª£ng text m·ªói batch (m·∫∑c ƒë·ªãnh 32).
        text_type: Lo·∫°i text, 'passage' (m·∫∑c ƒë·ªãnh) ho·∫∑c 'query'.
    
    Returns:
        List[List[float]]: Danh s√°ch c√°c vector embedding ƒë√£ chu·∫©n h√≥a.
    """
    # Th√™m prefix cho t·∫•t c·∫£ c√°c text
    prefix = f"{text_type}: "
    prefixed_texts = [prefix + t for t in texts]
    
    embeddings = []
    
    # Encode theo t·ª´ng batch
    for i in tqdm(range(0, len(prefixed_texts), batch_size)):
        batch = prefixed_texts[i:i + batch_size]
        
        # Encode batch, normalize embeddings
        batch_embeddings = model.encode(
            batch, 
            show_progress_bar=False, 
            normalize_embeddings=True
        )
        
        # L∆∞u k·∫øt qu·∫£
        embeddings.extend(batch_embeddings)
    
    # Chuy·ªÉn sang list of list
    return [emb.tolist() for emb in embeddings]

def generate_vietnamese_recipe_embeddings(df: pd.DataFrame):
    model = load_vietnamese_embedding_model()
    
    columns_to_embed = {
        "ingredient_names": "ingredient_names_embedding",
        "ingredient_quantities": "ingredient_quantities_embedding",
        "dish_name": "dish_name_embedding"
    }

    for text_col, out_col in columns_to_embed.items():
        print(f"Embedding column: {text_col}")
        texts = df[text_col].fillna("").astype(str).tolist()

        # G·ªçi embed_texts 1 l·∫ßn, tr·∫£ v·ªÅ list of list
        vectors = embed_texts(texts, model, batch_size=32)  # List[List[float]]

        # Tr·ª±c ti·∫øp l∆∞u list v√†o DataFrame, kh√¥ng c·∫ßn .tolist()
        df[out_col] = pd.Series(vectors, dtype=object)

    return df



if __name__ == "__main__":
    df = pd.read_csv("./data/test_501_1000_recipes.csv")

    df_emb = generate_vietnamese_recipe_embeddings(df)

    output = "./data/recipes_embeddings.pkl"
    df_emb.to_pickle(output)
    print(f"‚úÖ Saved embeddings to {output}")
