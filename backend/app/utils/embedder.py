# new_embedder.py
"""
Module há»— trá»£ load model embedding (BGE-M3) vÃ  encode vÄƒn báº£n.
"""

import os
import yaml
import torch
import numpy as np
from tqdm import tqdm
from sentence_transformers import SentenceTransformer

# ============================================
# 1. LOAD CONFIG
# ============================================

CONFIG_PATH = os.path.abspath(
    os.path.join(os.path.dirname(__file__), "../../config.yml")
)

with open(CONFIG_PATH, "r", encoding="utf-8") as f:
    config = yaml.safe_load(f)

EMBEDDING_CONFIG = config.get("embedding", {})
MODEL_NAME = EMBEDDING_CONFIG.get("model_name", "BAAI/bge-m3")  # máº·c Ä‘á»‹nh
BATCH_SIZE = EMBEDDING_CONFIG.get("batch_size", 32)             # máº·c Ä‘á»‹nh


# ============================================
# 2. LOAD EMBEDDING MODEL
# ============================================

def load_embedding_model(model_name=None, device=None):
    """
    Load embedding model SentenceTransformer (BGE-M3).

    Args:
        model_name (str): tÃªn model, náº¿u None dÃ¹ng config.
        device (str): 'cpu' hoáº·c 'cuda', náº¿u None tá»± detect.

    Returns:
        model: SentenceTransformer Ä‘Ã£ load vÃ  chuyá»ƒn device.
    """
    if model_name is None:
        model_name = MODEL_NAME

    print(f"ðŸ”¹ Loading embedding model: {model_name}")

    if device is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"

    model = SentenceTransformer(model_name)
    model.to(device)

    print(f"ðŸ‘‰ Using device: {device}")
    return model


# ============================================
# 3. EMBED TEXTS
# ============================================

def embed_texts(texts, model, batch_size=None, text_type=None):
    """
    Encode list text thÃ nh vector embedding.

    Args:
        texts (str | list[str]): vÄƒn báº£n hoáº·c list vÄƒn báº£n.
        model: SentenceTransformer Ä‘Ã£ load.
        batch_size (int): kÃ­ch thÆ°á»›c batch khi encode, default tá»« config.
        text_type: placeholder, Ä‘á»ƒ má»Ÿ rá»™ng náº¿u cáº§n xá»­ lÃ½ loáº¡i text khÃ¡c.

    Returns:
        np.ndarray: ma tráº­n embedding (float32), shape=(len(texts), embedding_dim)
    """
    if batch_size is None:
        batch_size = BATCH_SIZE

    if isinstance(texts, str):
        texts = [texts]

    vectors = []

    for i in tqdm(range(0, len(texts), batch_size), desc="Encoding batches"):
        batch = texts[i:i + batch_size]

        batch_vec = model.encode(
            batch,
            batch_size=batch_size,
            show_progress_bar=False,
            normalize_embeddings=True  # L2 normalize embeddings
        )

        vectors.extend(batch_vec)

    return np.array(vectors, dtype="float32")
